performanc
evaluation
information
retrieval
systems
many
slid
section
adapt
prof
joydeep
ghosh
ut
ece
turn
adapt
prof
dik
lee
univ
scienc
tech,
hong
kong
system
evaluation
•
many
retrieval
models/
algorithms/
systems,
one
best
•
best
component
:
–
ranking
function
dot
product,
cosine,
…
–
term
selection
stopword
removal,
stemming…
–
term
weighting
tf,
tf
idf,…
•
far
rank
list
will
user
need
look
find
/
relevant
documents
difficulti
evaluating
systems
•
effectiveness
relat
relevancy
retriev
items
•
relevancy
typically
binary
continuous
•
even
relevancy
binary,
can
difficult
judgment
mak
•
relevancy,
hum
standpoint,
:
–
subjective:
depends
upon
specific
user’s
judgment
–
situational:
relat
user’s
current
needs
–
cognitive:
depends
hum
perception
behavior
–
dynamic:
chang
tim
hum
label
corpor
gold
standard
•
start
corpus
documents
•
collect
set
queri
corpus
•
one
hum
experts
exhaustively
label
relevant
documents
query
•
typically
assum
binary
relevanc
judgments
•
requir
consider
hum
effort
larg
document/query
corpor
recall
numb
relevant
documents
retrievedtotal
numb
relevant
documents
precision
numb
relevant
documents
retrievedtotal
numb
documents
retriev
relevant
documents
retriev
documents
entir
document
collection
retriev
relevant
retriev
relevant
retriev
irrelevant
retriev
irrelevant
retriev
retriev
re
nt
irr
ev
t
precision
recall
precision
recall
•
precision
–
ability
retriev
top
rank
documents
mostly
relevant
•
recall
–
ability
search
find
relevant
items
corpus
determining
recall
difficult
•
total
numb
relevant
items
sometim
available:
–
sampl
across
datab
perform
relevanc
judgment
items
–
apply
different
retrieval
algorithms
datab
query
aggregat
relevant
items
tak
total
relevant
set
trad
recall
precision
recall
pr
ec
isi
ideal
returns
relevant
documents
miss
many
useful
ones
returns
relevant
documents
includ
lots
junk
computing
recall/precision
points
•
giv
query,
produc
rank
list
retrievals
•
adjusting
threshold
rank
list
produc
different
sets
retriev
documents,
therefor
different
recall/precision
measur
•
mark
document
rank
list
relevant
according
gold
standard
•
comput
recall/precision
pair
position
rank
list
contains
relevant
document
r
/
;
p
/
computing
recall/precision
points:
exampl
n
doc
relevant
x
x
x
x
x
let
total
relevant
docs
check
new
recall
point:
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
missing
one
relevant
document
nev
reach
recall
r
/
;
p
/
computing
recall/precision
points:
exampl
let
total
relevant
docs
check
new
recall
point:
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
interpolating
recall/precision
curv
•
interpolat
precision
valu
standard
recall
level:
–
rj
{
,
,
,
,
,
,
,
,
,
,
}
–
r
,
r
,
…,
r
•
interpolat
precision
j
th
standard
recall
level
maximum
known
precision
recall
level
j
th
j
th
level:
p
r
j
max
r
j≤r≤r
j
p
r
interpolating
recall/precision
curve:
exampl
recall
pr
ec
isi
interpolating
recall/precision
curve:
exampl
recall
pr
ec
isi
averag
recall/precision
curv
•
typically
averag
performanc
larg
set
queri
•
comput
averag
precision
standard
recall
level
across
queri
•
plot
averag
precision/recall
curv
evaluat
overall
system
performanc
document/query
corpus
compar
two
systems
•
curv
closest
upper
right
hand
corn
graph
indicat
best
performanc
recall
pr
ec
io
n
nostem
stem
sampl
rp
curv
cf
corpus
r
precision
•
precision
r
th
position
ranking
results
query
r
relevant
documents
n
doc
relevant
x
x
x
x
x
r
relevant
docs
r
precision
/
f
measur
•
one
measur
performanc
tak
account
recall
precision
•
harmonic
mean
recall
precision:
•
compar
arithmetic
mean,
need
high
harmonic
mean
high
f
prp
r
r
p
measur
parameteriz
f
measur
•
variant
f
measur
allows
weighting
emphasis
precision
recall:
•
valu

controls
trad
:


:
equally
weight
precision
recall
f


>
:
weight
recall


<
:
weight
precision
β
pr
βp
r
β
β
r
p
mean
averag
precision
map
•
averag
precision:
averag
precision
valu
points
relevant
document
retriev
–
ex:
/
–
ex:
/
•
mean
averag
precision:
averag
averag
precision
valu
set
queri
non
binary
relevanc
•
documents
rarely
entirely
relevant
non
relevant
query
•
many
sourc
grad
relevanc
judgments
–
relevanc
judgments
point
scal
–
multipl
judg
–
click
distribution
deviation
expect
levels
click
relevanc
judgments
cumulativ
gain
•
grad
relevanc
judgments,
can
comput
gain
rank
•
cumulativ
gain
rank
n:
reli
grad
relevanc
document
position
n
doc
relevanc
gain
cgn
discounting
bas
position
•
users
car
high
rank
documents,
discount
results
/log
rank
•
discount
cumulativ
gain:
n
doc
rel
gain
cgn
logn
dcgn
normaliz
discount
cumulativ
gain
ndcg
•
compar
dcgs,
normaliz
valu
ideal
ranking
normaliz
dcg
•
ideal
ranking:
n
doc
rel
gain
cgn
logn
dcgn
n
doc
rel
gain
cgn
logn
idcgn
normaliz
discount
cumulativ
gain
ndcg
•
normaliz
dcg
ideal
ranking:
•
ndcg
≤
ranks
•
ndcg
compar
across
different
queri
n
doc
rel
gain
dcgn
idcgn
ndcgn
issu
relevanc
•
marginal
relevance:
lat
documents
ranking
add
new
information
beyond
already
giv
high
documents
–
choic
retriev
set
encourag
diversity
novelty
•
coverag
ratio:
proportion
relevant
items
retriev
total
relevant
documents
known
user
prior
search
–
relevant
user
wants
locat
documents
seen
g
,
budget
report
year
factors
consid
•
user
effort:
work
requir
user
formulating
queries,
conducting
search,
screening
output
•
respons
time:
tim
interval
receipt
user
query
presentation
system
respons
•
form
presentation:
influenc
search
output
format
user’s
ability
utiliz
retriev
materials
•
collection
coverage:
extent
/
relevant
items
includ
document
corpus
/b
testing
deploy
system
•
can
exploit
existing
user
bas
provid
useful
feedback
•
randomly
send
small
fraction
−
incoming
users
variant
system
includ
singl
chang
•
judg
effectiveness
measuring
chang
clickthrough:
percentag
users
click
top
result
result
first
pag
experimental
setup
benchmarking
•
analytical
performanc
evaluation
difficult
document
retrieval
systems
many
characteristics
relevance,
distribution
words,
etc
,
difficult
describ
mathematical
precision
•
performanc
measur
benchmarking
,
retrieval
effectiveness
system
evaluat
giv
set
documents,
queries,
relevanc
judgments
•
performanc
dat
val
environment
system
evaluat
benchmarks
•
benchmark
collection
contains:
–
set
standard
documents
queries/topics
–
list
relevant
documents
query
•
standard
collections
traditional
:
–
smart
collection:
ftp://ftp
cs
cornell
edu/pub/smart
–
trec:
http://trec
nist
gov/
standard
document
collection
standard
queri
algorithm
test
evaluation
standard
result
retriev
result
precision
recall
benchmarking

problems
•
performanc
dat
val
particul
benchmark
•
building
benchmark
corpus
difficult
task
•
benchmark
web
corpor
just
starting
develop
•
benchmark
foreign
languag
corpor
just
starting
develop
•
previous
experiments
bas
smart
collection
fairly
small
ftp://ftp
cs
cornell
edu/pub/smart
collection
numb
numb
raw
siz
nam
documents
queri
mbytes
cacm
,
cisi
,
cran
,
med
,
tim
•
different
researchers
used
different
test
collections
evaluation
techniqu
early
test
collections
trec
benchmark
•
trec:
text
retrieval
conferenc
http://trec
nist
gov/
originat
tipst
program
sponsor
defens
advanc
research
projects
agency
darp
•
becam
annual
conferenc
,
co
sponsor
national
institut
standards
technology
nist
darp
•
participants
giv
parts
standard
set
documents
topics
queri
deriv
different
stag
training
testing
•
participants
submit
p/r
valu
final
document
query
corpus
present
results
conferenc
trec
objectiv
•
provid
common
ground
comparing
different
techniqu
–
set
documents
queries,
evaluation
method
•
sharing
resourc
experienc
developing
benchmark
–
major
sponsorship
government
develop
larg
benchmark
collections
•
encourag
participation
industry
academi
•
development
new
evaluation
techniques,
particularly
new
applications
–
retrieval,
routing/filtering,
non
english
collection,
web
bas
collection,
question
answering
trec
advantag
•
larg
scal
compar
mb
smart
collection
•
relevanc
judgments
provid
•
continuous
development
support
u
s
government
•
wid
participation:
–
trec
:
papers
pag
–
trec
:
papers
pag
–
trec
:
papers
pag
–
trec
:
papers
trec
tasks
•
ad
hoc:
new
questions
asked
static
set
dat
•
routing:
questions
asked,
new
information
search
news
clipping,
library
profiling
•
new
tasks
added
trec
interactive,
multilingual,
natural
language,
multipl
datab
merging,
filtering,
larg
corpus
gb,
million
documents
,
question
answering
characteristics
trec
collection
•
long
short
documents
hundr
one
thousand
uniqu
terms
document
•
test
documents
consist
:
wsj
wall
street
journal
articl
m
ap
associat
press
newswir
m
ziff
comput
select
disks
ziff
davis
publishing
m
fr
federal
regist
m
doe
abstracts
department
energy
reports
m
sampl
document
sgml
<doc>
<docno>
wsj
</docno>
<hl>
john
bla
near
accord
sell
unit,
sourc
say
</hl>
<dd>
//</dd>
<>
wall
street
journal
j
</>
<>
rel
tend
offers,
mergers,
acquisitions
tnm
marketing,
advertising
mkt
telecommunications,
broadcasting,
telephone,
telegraph
tel
</>
<dateline>
new
york
</dateline>
<text>
john
bla
amp;
co
clos
agreement
sell
tv
station
advertising
representation
operation
program
production
unit
investor
group
led
jam
h
rosenfield,
form
cbs
inc
executive,
industry
sourc
said
industry
sourc
put
valu
prop
acquisition
million
</text>
</doc>
sampl
query
sgml
<top>
<head>
tipst
topic
description
<num>
number:
<dom>
domain:
scienc
technology
<title>
topic:
natural
languag
processing
<desc>
description:
document
will
identify
type
natural
languag
processing
technology
develop
market
u
s
<narr>
narrative:
relevant
document
will
identify
company
institution
developing
marketing
natural
languag
processing
technology,
identify
technology,
identify
one
featur
company's
product
<>
concept
s
:
natural
languag
processing
;
translation,
language,
dictionary
<fac>
factor
s
:
<nat>
nationality:
u
s
</nat>
</fac>
<def>
definitions
s
:
</top>
trec
properti
•
documents
queri
contain
many
different
kinds
information
fields
•
generation
formal
queri
boolean,
vector
space,
etc
responsibility
system
–
system
may
good
querying
ranking,
generat
poor
queri
topic,
final
p/r
poor
evaluation
•
summary
tabl
statistics:
numb
topics,
numb
documents
retrieved,
numb
relevant
documents
•
recall
precision
average:
averag
precision
recall
levels
increments
•
document
level
average:
averag
precision
,
,
,
,
…
documents
retriev
•
averag
precision
histogram:
differenc
r
precision
topic
averag
r
precision
systems
topic
gov
web
corpus
•
recent
web
bas
gold
standard
corpus
assembl
nist
•
million
web
pag
gov
domain
–
high
proportion
gov
pag
•
total
gb
text
•
set
relevanc
judg
queri
cystic
fibrosis
cf
collection
•
,
abstracts
medical
journal
articl
cf
•
information
requests
queri
form
complet
english
questions
•
relevant
documents
determin
rat
separat
medical
experts
scale:
–
:
relevant
–
:
marginally
relevant
–
:
highly
relevant
cf
document
fields
•
medlin
access
numb
•
author
•
titl
•
sourc
•
major
subjects
•
minor
subjects
•
abstract
extract
•
referenc
documents
•
citations
document
sampl
cf
document
au
burnell
r
h
robertson
f
cystic
fibrosis
patient
kartagen
syndrom
j
dis
child
may
p
mj
cystic
fibrosis:
co
kartagen
triad:
co
mn
cas
report
chlorides:
hum
infant
lung:
ra
mal
situs
inversus:
co,
ra
sodium:
sweat:
ab
patient
exhibit
featur
kartagen
syndrom
cystic
fibrosis
,
authors'
knowledge,
represents
third
report
combination
cystic
fibrosis
exclud
diagnosis
kartagen
syndrom
mad
rf
kartagen
m
beitr
klin
tuberk
schwarz
v
arch
dis
child
mac
jw
clin
pediatr
…
ct
bochkov
dn
genetik
soviet
genetics
wood
re
rev
resp
dis
mossberg
b
mt
sinai
j
med
…
sampl
cf
queri
qn
qu
can
one
distinguish
effects
mucus
hypersecretion
infection
submucosal
glands
respiratory
tract
cf
nr
rd
qn
qu
lip
composition
cf
respiratory
secretions
nr
rd
nr:
numb
relevant
documents
rd:
relevant
documents
ratings
code:
four
ratings,
one
expert
preprocessing
vsr
experiments
•
separat
fil
document
just:
–
author
–
titl
–
major
minor
topics
–
abstract
extract
•
relevanc
judgment
mad
binary
assuming
documents
rat
expert
relevant
