
performance
evaluation
information
retrieval
systems
many
slides
section
adapted
prof
joydeep
ghosh
ut
ece
turn
adapted
prof
dik
lee
univ
science
tech,
hong
kong
system
evaluation
•
many
retrieval
models/
algorithms/
systems,
one
best
•
best
component
:
–
ranking
function
dot
product,
cosine,
…
–
term
selection
stopword
removal,
stemming…
–
term
weighting
tf,
tf
idf,…
•
far
ranked
list
will
user
need
look
find
/
relevant
documents
difficulties
evaluating
systems
•
effectiveness
related
relevancy
retrieved
items
•
relevancy
typically
binary
continuous
•
even
relevancy
binary,
can
difficult
judgment
make
•
relevancy,
human
standpoint,
:
–
subjective:
depends
upon
specific
user’s
judgment
–
situational:
relates
user’s
current
needs
–
cognitive:
depends
human
perception
behavior
–
dynamic:
changes
time
human
labeled
corpora
gold
standard
•
start
corpus
documents
•
collect
set
queries
corpus
•
one
human
experts
exhaustively
label
relevant
documents
query
•
typically
assumes
binary
relevance
judgments
•
requires
considerable
human
effort
large
document/query
corpora
recall
number
relevant
documents
retrievedtotal
number
relevant
documents
precision
number
relevant
documents
retrievedtotal
number
documents
retrieved
relevant
documents
retrieved
documents
entire
document
collection
retrieved
relevant
retrieved
relevant
retrieved
irrelevant
retrieved
irrelevant
retrieved
retrieved
re
le
nt
irr
ev
t
precision
recall
precision
recall
•
precision
–
ability
retrieve
top
ranked
documents
mostly
relevant
•
recall
–
ability
search
find
relevant
items
corpus
determining
recall
difficult
•
total
number
relevant
items
sometimes
available:
–
sample
across
database
perform
relevance
judgment
items
–
apply
different
retrieval
algorithms
database
query
aggregate
relevant
items
taken
total
relevant
set
trade
recall
precision
recall
pr
ec
isi
ideal
returns
relevant
documents
misses
many
useful
ones
returns
relevant
documents
includes
lots
junk
computing
recall/precision
points
•
given
query,
produce
ranked
list
retrievals
•
adjusting
threshold
ranked
list
produces
different
sets
retrieved
documents,
therefore
different
recall/precision
measures
•
mark
document
ranked
list
relevant
according
gold
standard
•
compute
recall/precision
pair
position
ranked
list
contains
relevant
document
r
/
;
p
/
computing
recall/precision
points:
example
n
doc
relevant
x
x
x
x
x
let
total
relevant
docs
check
new
recall
point:
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
missing
one
relevant
document
never
reach
recall
r
/
;
p
/
computing
recall/precision
points:
example
let
total
relevant
docs
check
new
recall
point:
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
r
/
;
p
/
interpolating
recall/precision
curve
•
interpolate
precision
value
standard
recall
level:
–
rj
{
,
,
,
,
,
,
,
,
,
,
}
–
r
,
r
,
…,
r
•
interpolated
precision
j
th
standard
recall
level
maximum
known
precision
recall
level
j
th
j
th
level:
p
r
j
max
r
j≤r≤r
j
p
r
interpolating
recall/precision
curve:
example
recall
pr
ec
isi
interpolating
recall/precision
curve:
example
recall
pr
ec
isi
average
recall/precision
curve
•
typically
average
performance
large
set
queries
•
compute
average
precision
standard
recall
level
across
queries
•
plot
average
precision/recall
curves
evaluate
overall
system
performance
document/query
corpus
compare
two
systems
•
curve
closest
upper
right
hand
corner
graph
indicates
best
performance
recall
pr
ec
io
n
nostem
stem
sample
rp
curve
cf
corpus
r
precision
•
precision
r
th
position
ranking
results
query
r
relevant
documents
n
doc
relevant
x
x
x
x
x
r
relevant
docs
r
precision
/
f
measure
•
one
measure
performance
takes
account
recall
precision
•
harmonic
mean
recall
precision:
•
compared
arithmetic
mean,
need
high
harmonic
mean
high
f
prp
r
r
p
e
measure
parameterized
f
measure
•
variant
f
measure
allows
weighting
emphasis
precision
recall:
•
value

controls
trade
:


:
equally
weight
precision
recall
e
f


>
:
weight
recall


<
:
weight
precision
e
β
pr
βp
r
β
β
r
p
mean
average
precision
map
•
average
precision:
average
precision
values
points
relevant
document
retrieved
–
ex:
/
–
ex:
/
•
mean
average
precision:
average
average
precision
value
set
queries
non
binary
relevance
•
documents
rarely
entirely
relevant
non
relevant
query
•
many
sources
graded
relevance
judgments
–
relevance
judgments
point
scale
–
multiple
judges
–
click
distribution
deviation
expected
levels
click
relevance
judgments
cumulative
gain
•
graded
relevance
judgments,
can
compute
gain
rank
•
cumulative
gain
rank
n:
reli
graded
relevance
document
position
n
doc
relevance
gain
cgn
discounting
based
position
•
users
care
high
ranked
documents,
discount
results
/log
rank
•
discounted
cumulative
gain:
n
doc
rel
gain
cgn
logn
dcgn
normalized
discounted
cumulative
gain
ndcg
•
compare
dcgs,
normalize
values
ideal
ranking
normalized
dcg
•
ideal
ranking:
n
doc
rel
gain
cgn
logn
dcgn
n
doc
rel
gain
cgn
logn
idcgn
normalized
discounted
cumulative
gain
ndcg
•
normalize
dcg
ideal
ranking:
•
ndcg
≤
ranks
•
ndcg
comparable
across
different
queries
n
doc
rel
gain
dcgn
idcgn
ndcgn
issues
relevance
•
marginal
relevance:
later
documents
ranking
add
new
information
beyond
already
given
higher
documents
–
choice
retrieved
set
encourage
diversity
novelty
•
coverage
ratio:
proportion
relevant
items
retrieved
total
relevant
documents
known
user
prior
search
–
relevant
user
wants
locate
documents
seen
e
g
,
budget
report
year
factors
consider
•
user
effort:
work
required
user
formulating
queries,
conducting
search,
screening
output
•
response
time:
time
interval
receipt
user
query
presentation
system
responses
•
form
presentation:
influence
search
output
format
user’s
ability
utilize
retrieved
materials
•
collection
coverage:
extent
/
relevant
items
included
document
corpus
/b
testing
deployed
system
•
can
exploit
existing
user
base
provide
useful
feedback
•
randomly
send
small
fraction
−
incoming
users
variant
system
includes
single
change
•
judge
effectiveness
measuring
change
clickthrough:
percentage
users
click
top
result
result
first
page
experimental
setup
benchmarking
•
analytical
performance
evaluation
difficult
document
retrieval
systems
many
characteristics
relevance,
distribution
words,
etc
,
difficult
describe
mathematical
precision
•
performance
measured
benchmarking
,
retrieval
effectiveness
system
evaluated
given
set
documents,
queries,
relevance
judgments
•
performance
data
valid
environment
system
evaluated
benchmarks
•
benchmark
collection
contains:
–
set
standard
documents
queries/topics
–
list
relevant
documents
query
•
standard
collections
traditional
:
–
smart
collection:
ftp://ftp
cs
cornell
edu/pub/smart
–
trec:
http://trec
nist
gov/
standard
document
collection
standard
queries
algorithm
test
evaluation
standard
result
retrieved
result
precision
recall
benchmarking

problems
•
performance
data
valid
particular
benchmark
•
building
benchmark
corpus
difficult
task
•
benchmark
web
corpora
just
starting
developed
•
benchmark
foreign
language
corpora
just
starting
developed
•
previous
experiments
based
smart
collection
fairly
small
ftp://ftp
cs
cornell
edu/pub/smart
collection
number
number
raw
size
name
documents
queries
mbytes
cacm
,
cisi
,
cran
,
med
,
time
•
different
researchers
used
different
test
collections
evaluation
techniques
early
test
collections
trec
benchmark
•
trec:
text
retrieval
conference
http://trec
nist
gov/
originated
tipster
program
sponsored
defense
advanced
research
projects
agency
darpa
•
became
annual
conference
,
co
sponsored
national
institute
standards
technology
nist
darpa
•
participants
given
parts
standard
set
documents
topics
queries
derived
different
stages
training
testing
•
participants
submit
p/r
values
final
document
query
corpus
present
results
conference
trec
objectives
•
provide
common
ground
comparing
different
techniques
–
set
documents
queries,
evaluation
method
•
sharing
resources
experiences
developing
benchmark
–
major
sponsorship
government
develop
large
benchmark
collections
•
encourage
participation
industry
academia
•
development
new
evaluation
techniques,
particularly
new
applications
–
retrieval,
routing/filtering,
non
english
collection,
web
based
collection,
question
answering
trec
advantages
•
large
scale
compared
mb
smart
collection
•
relevance
judgments
provided
•
continuous
development
support
u
s
government
•
wide
participation:
–
trec
:
papers
pages
–
trec
:
papers
pages
–
trec
:
papers
pages
–
trec
:
papers
trec
tasks
•
ad
hoc:
new
questions
asked
static
set
data
•
routing:
questions
asked,
new
information
searched
news
clipping,
library
profiling
•
new
tasks
added
trec
interactive,
multilingual,
natural
language,
multiple
database
merging,
filtering,
large
corpus
gb,
million
documents
,
question
answering
characteristics
trec
collection
•
long
short
documents
hundred
one
thousand
unique
terms
document
•
test
documents
consist
:
wsj
wall
street
journal
articles
m
ap
associate
press
newswire
m
ziff
computer
select
disks
ziff
davis
publishing
m
fr
federal
register
m
doe
abstracts
department
energy
reports
m
sample
document
sgml
<doc>
<docno>
wsj
</docno>
<hl>
john
blair
near
accord
sell
unit,
sources
say
</hl>
<dd>
//</dd>
<>
wall
street
journal
j
</>
<>
rel
tender
offers,
mergers,
acquisitions
tnm
marketing,
advertising
mkt
telecommunications,
broadcasting,
telephone,
telegraph
tel
</>
<dateline>
new
york
</dateline>
<text>
john
blair
amp;
co
close
agreement
sell
tv
station
advertising
representation
operation
program
production
unit
investor
group
led
james
h
rosenfield,
former
cbs
inc
executive,
industry
sources
said
industry
sources
put
value
proposed
acquisition
million
</text>
</doc>
sample
query
sgml
<top>
<head>
tipster
topic
description
<num>
number:
<dom>
domain:
science
technology
<title>
topic:
natural
language
processing
<desc>
description:
document
will
identify
type
natural
language
processing
technology
developed
marketed
u
s
<narr>
narrative:
relevant
document
will
identify
company
institution
developing
marketing
natural
language
processing
technology,
identify
technology,
identify
one
features
company's
product
<>
concept
s
:
natural
language
processing
;
translation,
language,
dictionary
<fac>
factor
s
:
<nat>
nationality:
u
s
</nat>
</fac>
<def>
definitions
s
:
</top>
trec
properties
•
documents
queries
contain
many
different
kinds
information
fields
•
generation
formal
queries
boolean,
vector
space,
etc
responsibility
system
–
system
may
good
querying
ranking,
generates
poor
queries
topic,
final
p/r
poor
evaluation
•
summary
table
statistics:
number
topics,
number
documents
retrieved,
number
relevant
documents
•
recall
precision
average:
average
precision
recall
levels
increments
•
document
level
average:
average
precision
,
,
,
,
…
documents
retrieved
•
average
precision
histogram:
difference
r
precision
topic
average
r
precision
systems
topic
gov
web
corpus
•
recent
web
based
gold
standard
corpus
assembled
nist
•
million
web
pages
gov
domain
–
high
proportion
gov
pages
•
total
gb
text
•
set
relevance
judged
queries
cystic
fibrosis
cf
collection
•
,
abstracts
medical
journal
articles
cf
•
information
requests
queries
form
complete
english
questions
•
relevant
documents
determined
rated
separate
medical
experts
scale:
–
:
relevant
–
:
marginally
relevant
–
:
highly
relevant
cf
document
fields
•
medline
access
number
•
author
•
title
•
source
•
major
subjects
•
minor
subjects
•
abstract
extract
•
references
documents
•
citations
document
sample
cf
document
au
burnell
r
h
robertson
e
f
ti
cystic
fibrosis
patient
kartagener
syndrome
j
dis
child
may
p
mj
cystic
fibrosis:
co
kartagener
triad:
co
mn
case
report
chlorides:
human
infant
lung:
ra
male
situs
inversus:
co,
ra
sodium:
sweat:
ab
patient
exhibited
features
kartagener
syndrome
cystic
fibrosis
,
authors'
knowledge,
represents
third
report
combination
cystic
fibrosis
excluded
diagnosis
kartagener
syndrome
made
rf
kartagener
m
beitr
klin
tuberk
schwarz
v
arch
dis
child
mace
jw
clin
pediatr
…
ct
bochkova
dn
genetika
soviet
genetics
wood
re
rev
respir
dis
mossberg
b
mt
sinai
j
med
…
sample
cf
queries
qn
qu
can
one
distinguish
effects
mucus
hypersecretion
infection
submucosal
glands
respiratory
tract
cf
nr
rd
qn
qu
lipid
composition
cf
respiratory
secretions
nr
rd
nr:
number
relevant
documents
rd:
relevant
documents
ratings
code:
four
ratings,
one
expert
preprocessing
vsr
experiments
•
separate
file
document
just:
–
author
–
title
–
major
minor
topics
–
abstract
extract
•
relevance
judgment
made
binary
assuming
documents
rated
expert
relevant
